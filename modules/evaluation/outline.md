### 1. Introduction to Generative AI Evaluation

- **Why Evaluate Generative AI Applications?**
  - Importance of evaluation for quality, safety, governance, and ethical use.
- **Resources**
  - [Evaluating Generative AI](https://blog.palantir.com/evaluating-generative-ai-a-field-manual-0cdaf574a9e1)
  - [Evaluation and monitoring metrics for generative AI](https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/evaluation-metrics-built-in)

---

### 2. Differences Between Evaluating Traditional ML Models, Generative AI, and Agentic AI

- **Traditional ML Models**
  - Focus on metrics like accuracy, precision, recall, and F1 score.
  - Example: Traditional ML: Evaluating a spam classifier.
- **Generative AI (LLMs)**
  - Emphasize text generation quality, coherence, relevance, and creativity.
  - Example: Generative AI assessing the quality of text generated by an LLM.
- **Agentic AI (Autonomous Agents)**
  - Evaluate task completion, reasoning, planning, tool use, and autonomy.
  - Example: Agentic AI measuring the performance of an autonomous research assistant.
_ **Resources**
  - [Evaluation and monitoring metrics for generative AI](https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/evaluation-metrics-built-in)
  - [How to Evaluate Generative AI Models: Key Metrics and Best Practices](https://www.datastax.com/guides/how-to-evaluate-generative-ai-models)
  - [Measuring Success: A Deep Dive into Agentic AI System Evaluation](https://www.linkedin.com/pulse/measuring-success-deep-dive-agentic-ai-system-evaluation-soma-sec5c)
---

### 3. Basics of Evaluating Generative AI Outputs

- **Types of Outputs and Evaluation Challenges**
  - Diversity of outputs (text, image, agentic behavior) and unique evaluation challenges.
- **Quantitative Metrics**
  - Text: BLEU, ROUGE, perplexity, log-likelihood.
  - Images: Inception Score (IS), Frechet Inception Distance (FID), SSIM, PSNR.
  - Diversity and creativity metrics (e.g., Self-BLEU).
- **Qualitative Evaluation**
  - Human evaluation for coherence, creativity, bias, and factual accuracy.
- **Resources**:
  - [What is a BLEU score?](https://learn.microsoft.com/en-us/azure/ai-services/translator/custom-translator/concepts/bleu-score)
  - [Evaluate generative AI applications](https://learn.microsoft.com/en-us/training/paths/evaluate-generative-ai-apps/)

---

### 4. Advanced Topics: Evaluation of Agentic AI Applications

- **What are Agentic AI Applications?**
  - AI systems with autonomous decision-making and multi-stage reasoning.
- **Evaluation Techniques for Agentic Systems**
  - Multi-stage reasoning chains and agent behavior assessment.
  - Custom metrics for task-specific goals and performance.
  - Offline vs. online evaluation strategies.
  - Example: An agentic customer service AI that autonomously retrieves documents, processes claims, and schedules follow-ups must be evaluated for correctness across all steps, not just the final output.
- **Key Metrics**
  - Tool Utilization Efficacy (TUE): How effectively the agent uses external tools.
  - Memory Coherence and Retrieval (MCR): Accuracy and relevance of information recalled.
  - Strategic Planning Index (SPI): Quality of multi-step plans.
  - Component Synergy Score (CSS): How well different agent components work together.
- **Security, Guardrails, and Governance**
  - Implementing and testing guardrails to ensure safe and ethical outputs.
- **Resources**
  - [A Holistic 8-Step Framework for Evaluating Agentic AI Systems](https://raga.ai/research/a-holistic-8-step-framework-for-evaluating-agentic-ai-systems)
  - [Measuring Success: A Deep Dive into Agentic AI System Evaluation](https://www.linkedin.com/pulse/measuring-success-deep-dive-agentic-ai-system-evaluation-soma-sec5c)

---

### 5. Advanced Techniques

- **Multi-Tier Quality Assessment Framework**
  - Combining automated metrics with human-in-the-loop review for nuanced feedback.
- **Domain-Specific Metrics**
  - Tailoring evaluation criteria to industry needs (e.g., legal accuracy, clinical relevance).
_ **_Continuous Feedback and Model Improvement**
  - Using real-world user feedback and retraining for ongoing quality enhancement.
- **Ethical Considerations**
  - Bias detection, privacy, responsible use, and environmental impact in evaluation.
- **Resources**
  - [How to Evaluate Generative AI Models: Key Metrics and Best Practices](https://www.datastax.com/guides/how-to-evaluate-generative-ai-models)
  - [Evaluation and monitoring metrics for generative AI](https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/evaluation-metrics-built-in)

---

### 6. Tools and Frameworks for Evaluation

- **Evaluation Platforms and SDKs**
  - LangChain `agent-eval`: Purpose-built for evaluating agentic workflows. Supports metrics like task success, step count, tool usage, and LLM-as-a-judge evaluations.
  - OpenAI `evals`: Modular framework for evaluating LLMs and agents. Supports custom metrics, ground truth comparisons, and LLM-based grading.
  - Azure AI Evaluation SDK for synthetic data generation and output assessment.
  - Weights \& Biases for experiment tracking and debugging generative models.
  - G-Eval / MT-Bench / AlpacaEval: Use LLMs to evaluate coherence, relevance, factuality, and plan quality.

- **Safety and Bias Detection**: 
  - Perspective API 
  - Detoxify - Evaluate toxicity and bias in agent outputs.

- **Resources**:
- [Azure Evaluation SDK](https://learn.microsoft.com/en-us/python/api/overview/azure/ai-evaluation-readme)
- [Azure Project SDK](https://learn.microsoft.com/en-us/python/api/overview/azure/ai-projects-readme)
- [Azure AI Foundry Evaluations](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/flow-develop-evaluation)
- [OpenAI `evals`](https://github.com/openai/evals)
- [Weights & Biases](https://wandb.ai/)
- [Perspective API](https://www.perspectiveapi.com/)
- [LangChain `agent-eval`](https://langchain.com/docs/evaluation)

---

### 7. Hands-on Exercises and Learning Resources

- **Team Exercise**
  - TODO: Pick Scenario
  - Implement simple (agentic) solution - (use Kingfard's?)
  - Create Synthetic Evaluation Dataset
  - Create Evaluations in Azure AI Foundry
  - Create Evaluations using Azure Evaluation SDK
  - Create Evaluations using the Azure Project SDK
  - Evaluate with Real-World Data
  - Evaluate Content Safety
  - Develop an Evaluation Flow

- **Resources**
  - [Microsoft Learn Learning Path on Evaluating Generative AI Applications with Azure AI Foundry](https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/evaluation-metrics-built-in)
  - [Azure AI Evaluation SDK (preview) with samples](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/develop/agent-evaluate-sdk)
  - [Weights \& Biases tutorials](https://docs.wandb.ai/tutorials/)
  - [Develop an evaluation flow in Azure AI Foundry portal](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/flow-develop-evaluation)
  - [Evaluate your AI agents locally with Azure AI Evaluation SDK (preview)](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/develop/agent-evaluate-sdk)
  - [Evaluate your Generative AI application on the cloud with Azure AI Projects SDK (preview)](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/develop/cloud-evaluation)

---