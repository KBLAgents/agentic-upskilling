### 1. Introduction to Generative AI Evaluation

- **Why Evaluate Generative AI Applications?**
Importance of evaluation for quality, safety, governance, and ethical use.

---

### 2. Differences Between Evaluating Traditional ML Models, Generative AI, and Agentic AI

- **Traditional ML Models**: Focus on metrics like accuracy, precision, recall, and F1 score.
- **Generative AI (LLMs)**: Emphasize text generation quality, coherence, relevance, and creativity.
- **Agentic AI (Autonomous Agents)**: Evaluate task completion, reasoning, planning, tool use, and autonomy.

- **Examples**:
- Traditional ML: Evaluating a spam classifier.
- Generative AI: Assessing the quality of text generated by GPT-3.
- Agentic AI: Measuring the performance of an autonomous research assistant.

- **Resources**:
- TBD

---

### 3. Basics of Evaluating Generative AI Outputs

- **Types of Outputs and Evaluation Challenges**
Diversity of outputs (text, image, agentic behavior) and unique evaluation challenges.
- **Quantitative Metrics**
- Text: BLEU, ROUGE, perplexity, log-likelihood[^3].
- Images: Inception Score (IS), Frechet Inception Distance (FID), SSIM, PSNR.
- Diversity and creativity metrics (e.g., Self-BLEU)[^3].
- **Qualitative Evaluation**
Human evaluation for coherence, creativity, bias, and factual accuracy.

- **Resources**:
- TBD

---

### 4. Advanced Topics: Evaluation of Agentic AI Applications

- **What are Agentic AI Applications?**
AI systems with autonomous decision-making and multi-stage reasoning.
- **Evaluation Techniques for Agentic Systems**
- Multi-stage reasoning chains and agent behavior assessment.
- Custom metrics for task-specific goals and performance.
- Offline vs. online evaluation strategies.
- **Security, Guardrails, and Governance**
Implementing and testing guardrails to ensure safe and ethical outputs.

- **Resources**:
- TBD

---

## Advanced Techniques

- **Multi-Tier Quality Assessment Framework**
Combining automated metrics with human-in-the-loop review for nuanced feedback.
- **Domain-Specific Metrics**
Tailoring evaluation criteria to industry needs (e.g., legal accuracy, clinical relevance).
- **Continuous Feedback and Model Improvement**
Using real-world user feedback and retraining for ongoing quality enhancement.
- **Ethical Considerations**
Bias detection, privacy, responsible use, and environmental impact in evaluation.

- **Resources**:
- TBD

---

### 5. Tools and Frameworks for Evaluation

- **Evaluation Platforms and SDKs**
    - LangChain `agent-eval`: Purpose-built for evaluating agentic workflows. Supports metrics like task success, step count, tool usage, and LLM-as-a-judge evaluations.
    - OpenAI `evals`: Modular framework for evaluating LLMs and agents. Supports custom metrics, ground truth comparisons, and LLM-based grading.
    - Azure AI Evaluation SDK for synthetic data generation and output assessment.
    - Weights \& Biases for experiment tracking and debugging generative models.
    - G-Eval / MT-Bench / AlpacaEval: Use LLMs to evaluate coherence, relevance, factuality, and plan quality.

- **Safety and Bias Detection**: Perspective API / Detoxify - Evaluate toxicity and bias in agent outputs.

- **Resources**:
- [Azure Evaluation SDK](https://learn.microsoft.com/en-us/python/api/overview/azure/ai-evaluation-readme)
- [Azure Project SDK](https://learn.microsoft.com/en-us/python/api/overview/azure/ai-projects-readme)
- [Azure AI Foundry Evaluations](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/flow-develop-evaluation)
- [OpenAI `evals`](https://github.com/openai/evals)
- [Weights & Biases](https://wandb.ai/)
- [Perspective API](https://www.perspectiveapi.com/)
- [LangChain `agent-eval`](https://langchain.com/docs/evaluation)

---

### 6. Hands-on Exercises and Learning Resources

- **Team Exercise**
    - TODO: Pick Scenario
    - Implement simple (agentic) solution - (use Kingfard's?)
    - Create Synthetic Evaluation Dataset
    - Create Evaluations in Azure AI Foundry
    - Create Evaluations using Azure Evaluation SDK [^10]
    - Create Evaluations using the Azure Project SDK [^11]
    - Evaluate with Real-World Data [??] 
    - Evaluate Content Safety [??]
    - Develop an Evaluation Flow [^12]

- **Learning Resources**

    - Microsoft Learn Learning Path on Evaluating Generative AI Applications with Azure AI Foundry[^9].
    - Azure AI Evaluation SDK (preview) with samples [^4]
    - Weights \& Biases tutorials for model debugging and experiment tracking[^2].

---

This structured module covers foundational knowledge, advanced evaluation of agentic AI, practical best practices, and hands-on experience with leading tools and frameworks, supported by curated learning resources and exercises to build expertise in generative AI evaluation.

<div style="text-align: center">‚ÅÇ</div>

[^2]: https://www.deeplearning.ai/short-courses/evaluating-debugging-generative-ai/
[^3]: https://learn.microsoft.com/en-us/training/modules/evaluate-generative-ai-apps/
[^4]: https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/develop/agent-evaluate-sdk
[^7]: https://www.dataforce.ai/blog/building-better-ai-best-practices-generative-ai-quality-rating
[^9]: https://learn.microsoft.com/en-us/training/paths/evaluate-generative-ai-apps/
[^10]: https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/develop/agent-evaluate-sdk
[^11]: https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/develop/cloud-evaluation
[^12]: https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/flow-develop-evaluation